# 分布式面试题



**ACID 是数据库事务完整性的理论，CAP 是分布式系统设计理论，BASE 是 CAP 理论中 AP 方案的延伸。**

## CAP 

### 理论

<img src="https://cdn.jsdelivr.net/gh/Miyuki7/image-host/blog-imgimage-20220829111838788.png" alt="image-20220829111838788" style="zoom:50%;" />

Consistency、Availability、Partition Tolerance

一致性、可用性、分区容错性

---

> **不是所谓的 “3选2”**
>
> 正确理解应当是： <u>CAP 理论中的分区容错性 P 是一定要满足的，在此基础上，只能满足可用性 A 或者一致性 C。</u>
>
> 也就是说，
>
> 当发生网络分区的时候，如果我们要继续提供服务，那么强一致性和可用性只能 2 选 1。
>
> 如果系统没有发生发生分区，也就不存在 P，这个时候我们就要思考如何同时保证 CA。

CAP 理论指出，对一个分布式系统来说，当设计读写操作时，只能同时满足三点中的两点

* 一致性：所有节点访问同一份最新的数据副本
* 可用性：非故障节点在合理的时间内返回合理的象形
* 分区容错性：分布式系统出现网络分区的时候仍然能够对外提供服务

> 网络分区：
>
> 分布式系统中，多个节点之间本来是连通的，但是因为某些故障，某些节点之间不联通了，整个网络就分成了几个区域

选择的关键在于具体的业务场景，比如需要确保强一致性的场景银行，一般会选择 CP。

### 应用场景

以注册中心为例，探讨 CAP 的实际应用。

常见的可以作为注册中心的组件有：

* Zookeeper
  * 保证的是 CP。任何时刻对 Zookeeper 的读请求都能得到一致性的结果。但是 Zookeeper 不能保证每次请求的可用性，比如在 Leader 的选举过程中或者半数以上机器不可用的时候，服务就是不可用的
* Eureka
  * 保证的是 AP。在 Eureka 中，不存在 Leader 节点，每个节点都是一样的。因此 Eureka 中不会存在选举过程或者半数以上机器不可用服务不可用的情况
  * Eureka 保证的是，即时大部分节点挂掉，也不会影响正常提供服务，只要有一个节点是可用的就行了。只不过这个结点上的数据可能并不是最新的
* Nacos
  * 不仅支持 AP，也支持 CP

![image-20220829112358807](https://cdn.jsdelivr.net/gh/Miyuki7/image-host/blog-imgimage-20220829112358807.png)



## Base 理论

> **BASE** 是 **Basically Available（基本可用）** 、**Soft-state（软状态）** 和 **Eventually Consistent（最终一致性）** 三个短语的缩写。
>
> BASE 理论是对 CAP 中一致性 C 和可用性 A <u>权衡的结果</u>，其来源于对大规模互联网系统分布式实践的总结，是基于 CAP 定理逐步演化而来的，它大大降低了我们对系统的要求。



### 核心思想

即时无法做到强一致性，但每个应用都可以根据自身业务特点，采用适当的方式来使系统**达到最终一致性**



### 理论

<img src="https://cdn.jsdelivr.net/gh/Miyuki7/image-host/blog-imgimage-20220829113027866.png" alt="image-20220829113027866" style="zoom:50%;" />



* 基本可用
  * 分布式系统出现不可预知的故障的时候，允许损失部分可用性。
    * 响应时间上的损失
      * 原本处理用户请求需要 1 秒返回结果，但是系统出现故障，处理用户请求的时间变为 3 秒
    * 系统功能上的损失
      * 正常情况下可以使用系统全部功能，但是当系统访问量剧增，系统的非核心部分功能无法使用
* 软状态
  * 允许系统中的数据存在中间状态（CAP 理论中的数据不一致），并认为该中间状态的存在不会影响系统整体可用性。
  * 也就是说，允许系统在不同节点的数据副本同步的过程中，存在延时
* 最终一致性
  * 强调的是，系统中的所有数据副本，在经过一段时间的同步后，最终，能达到一个一致的状态。
  * 最终一致性本质是确保系统最终数据能够达到一致，而不需要确保系统数据的强一致性

> 分布式一致性的 3 种级别
>
> * 强一致性：系统写入了什么，读出来就是什么
> * 弱一致性：不一定读取到最新写入的值，也不保证多少时间之后读取到的值是最新的
> * 最终一致性：系统会保证，一定时间内达到数据的一致性状态



## 分布式事务

> 数据库自身提供的本地事务机制无法确保对多数据源全局操作的可靠性

* 在分布式事务模型中，一个资源管理器 管理多个 事务管理器，即一个服务程序访问多个数据源
* 资源管理器 是一个全局事务管理器，协调多方本地事务的进度，使其共同提交或回滚。最终达成一种全局的 ACID



### 二将军问题，幂等性

> 一支白军被围困在一个山谷中，山谷的左右两侧是蓝军。困在山谷中的白军人数多于山谷两侧的任意一支蓝军，而少于两支蓝军的之和。若一支蓝军对白军单独发起进攻，则必败无疑；但若两支蓝军同时发起进攻，则可取胜。两只蓝军的总指挥位于山谷左侧，他希望两支蓝军同时发起进攻，这样就要把命令传到山谷右侧的蓝军，以告知发起进攻的具体时间。假设他们只能派遣士兵穿越白军所在的山谷（唯一的通信信道）来传递消息，那么在穿越山谷时，士兵有可能被俘虏。
>
> 只有当送信士兵成功往返后，总指挥才能确认这场战争的胜利（上方图）。现在问题来了，派遣出去送信的士兵没有回来，则左侧蓝军中的总指挥能不能决定按命令中约定的时间发起进攻？
>
> 答案是不确定，派遣出去送信的士兵没有回来，他可能遇到两种状况：
>
> - 命令还没送达就被俘虏了，这时候右侧蓝军根本不知道要何时进攻；
> - 命令送达，但返回途中被俘虏了，这时候右侧蓝军知道要何时进攻，但左侧蓝军不知道右侧蓝军是否知晓进攻时间。

总结： 客户端发出的请求没有得到响应。为了确保服务端成功写入数据，客户端只能重发请求，直到接受到服务器端的响应。

但这往往又会导致消息的重复发送。

**因此要保证一次事务中的请求无论被发送多少次，接收方有且只有一次执行指定动作。**

这种机制叫做接收方的幂等性



### 2 PC 

2 PC （两阶段提交）是一种实现分布式事务的简单模型。

* 准备阶段
  * 事务的协调者向各个事务参与者发起询问请求：”我要执行全局事务了，这个事务涉及到的资源分布在......，你们各自准备好资源（各自执行事务到待提交阶段）“
  * 各个参与者向协调者回复（准备好，允许提交全局事务）或者（本参与者无法拿到全局事务所需的本地资源，被其他本地事务锁住或超时）
* 提交阶段
  * 如果各个参与者都准备好，则协调者向所有参与者发起事务提交，然后所有参与者收到后各自执行本地事务提交操作并向协调者发送 ACK
  * 如果任何一个参与者没有准备好或者超时，则协调者向所有参与者发送事务回滚，然后所有参与者收到后执行本地事务回滚并向协调者发送 ACK



![image-20220829144756485](https://cdn.jsdelivr.net/gh/Miyuki7/image-host/blog-imgimage-20220829144756485.png)

---



2 PC 存在如下问题

* 性能差
  * **准备阶段要等到所有参与者返回才能进入阶段二**。在这个期间各个参与者相关资源被排他锁锁住，意图使用这些资源的本地事务只能等待。**影响了各个参与者的本地事务并发度**
  * 准备阶段完成后，**如果协调者宕机，所有的参与者都得不到回滚或者提交指令**
  * 提交阶段，协调者向所有的参与者发送提交指令，**如果一个参与者没有回复 ACK，那么协调者不知道指令是否被参与者收到**，也就无法决定下一步是否进行全体参与者的回滚

### 3 PC

> 询问阶段 + 准备阶段 + 提交或回滚阶段
>
> 利用超时机制解决了 2 PC 的同步阻塞问题，避免资源被永久锁定，进一步加强了整个事务过程的可靠性。
>
> 但是 3 PC 同样无法应对类似的宕机问题，只不过多数据源中数据不一致问题的概率更小。

与 2 PC 相比，三阶段提交有两个改动点

* 引入超时机制。同时在协调者和参与者中引入超时机制
* 在第一阶段和第三阶段中插入一个准备阶段。保证了在最后阶段之前各参与节点的状态是一致的。参与者接收到preCommit 请求后，会执行事务操作，并将Undo和Redo信息记录到事务日志中。

![image-20220829145758130](https://cdn.jsdelivr.net/gh/Miyuki7/image-host/blog-imgimage-20220829145758130.png)

相对于 2 PC，3 PC 主要解决了**协调者宕机之后事务阻塞的问题**，**因为一旦参与者无法及时收到来自协调者的信息之后，他会默认执行commit。而不会一直持有事务资源并处于阻塞状态。**但是这种机制也会导致**数据一致性**问题，因为，由于网络原因，协调者发送的abort响应没有及时被参与者接收到，那么参与者在等待超时之后执行了commit操作。这样就和其他接到abort命令并执行回滚的参与者之间存在数据不一致的情况。



